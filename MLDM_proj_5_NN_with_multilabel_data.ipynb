{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLDM-proj-5 NN with multilabel data",
      "provenance": [],
      "collapsed_sections": [
        "PDgtUSo-2Pa_",
        "phQ6oAOqLAVv"
      ],
      "authorship_tag": "ABX9TyOlNfxLKzcjjByXpYOa7POm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreyKuratov/project_mldm_21/blob/main/MLDM_proj_5_NN_with_multilabel_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8rX5ePYxoD2",
        "outputId": "73c3bcdf-4ade-4eb3-f075-f2d290471e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import scipy.signal as signal\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import librosa as lb \n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from skimage.transform import resize\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from skimage.transform import resize\n",
        "\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "3FVPSiKXxu3W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Загрузка файлов и подготовка к обучению"
      ],
      "metadata": {
        "id": "PDgtUSo-2Pa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DIR = '/content/drive/MyDrive/mldm21_project/'\n",
        "PATH_TO_SAVE = '/content/drive/MyDrive/mldm21_project/'\n",
        "\n",
        "data_tp = np.load(PATH_TO_SAVE+'sr_48e3_nfft_2e11_hlop_2e9_pict_256x512_u8_tp.npz',allow_pickle=True)\n",
        "# 'imgs' 'labels'\n",
        "data_fp = np.load(PATH_TO_SAVE+'sr_48e3_nfft_2e11_hlop_2e9_pict_256x512_u8_fp.npz',allow_pickle=True)\n",
        "# 'imgs' 'labels'\n",
        "\n",
        "data_majorTest = np.load(PATH_TO_SAVE+'sr_48e3_nfft_2e11_hlop_2e9_pict_256x512_u8_majorTest.npz',allow_pickle=True)\n",
        "# 'imgs' 'labels'\n",
        "labels = np.load(PATH_TO_SAVE+'labels_simple_v2.npz')\n",
        "# 'files_fp' 'labels_fp' 'files_tp' 'labels_tp'\n",
        "\n",
        "train_files = np.load('/content/drive/MyDrive/mldm21_project/train_ids_0.npy',allow_pickle=True) \n",
        "test_files = np.load('/content/drive/MyDrive/mldm21_project/test_ids_0.npy',allow_pickle=True) \n",
        "\n",
        "pd_fp = pd.read_csv('/content/drive/MyDrive/mldm21_project/train_fp.csv')\n",
        "pd_tp = pd.read_csv('/content/drive/MyDrive/mldm21_project/train_tp.csv')"
      ],
      "metadata": {
        "id": "udS5W2u3x2N3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание обучающего и тестового набора данных"
      ],
      "metadata": {
        "id": "28nyKwlE2SNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcQlM8B6e6pW",
        "outputId": "dd879fb2-dd54-4506-b8c0-7f434ec8edb2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1132, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_test = np.isin(data_tp['labels'], test_files)\n",
        "mask_train = ~np.isin(data_tp['labels'], test_files)"
      ],
      "metadata": {
        "id": "xcaJN2G02X15"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATA_TP = data_tp['imgs'][mask_train]\n",
        "TRAIN_LABEL_TP = labels['labels_tp'][mask_train]\n",
        "TRAIN_DATA_FP = data_fp['imgs']\n",
        "TRAIN_LABEL_FP = labels['labels_fp']\n",
        "\n",
        "TRAIN_DATA = np.concatenate((TRAIN_DATA_TP,TRAIN_DATA_FP),axis=0)\n",
        "TRAIN_LABEL = np.concatenate((TRAIN_LABEL_TP,TRAIN_LABEL_FP),axis=0).copy()\n",
        "\n",
        "TEST_DATA = data_tp['imgs'][mask_test]\n",
        "TEST_LABEL = labels['labels_tp'][mask_test].copy()\n",
        "\n",
        "TRAIN_DATA = np.swapaxes(TRAIN_DATA,1,3).copy()\n",
        "TEST_DATA = np.swapaxes(TEST_DATA,1,3).copy()\n"
      ],
      "metadata": {
        "id": "iOa0F4lyg08u"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAJOR_TEST = np.swapaxes(data_majorTest['imgs'],1,3).copy()\n",
        "MAJOR_FILES = data_majorTest['labels']"
      ],
      "metadata": {
        "id": "Kz62JhpJKZy-"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохранение данных на будущее"
      ],
      "metadata": {
        "id": "k5AbhtALJNBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_SAVE = '/content/drive/MyDrive/mldm21_project/'\n",
        "np.savez(PATH_TO_SAVE+'pure_train',data=TRAIN_DATA,labels=TRAIN_LABEL)\n",
        "np.savez(PATH_TO_SAVE+'pure_test',data=TEST_DATA,labels=TEST_LABEL)\n",
        "np.savez(PATH_TO_SAVE+'MAJOR_test', data =MAJOR_TEST, files=MAJOR_FILES)"
      ],
      "metadata": {
        "id": "eS8Jv8OAiXMG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wkVnPcZKKPJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Функции для загрузки теста и трейна и датасеты"
      ],
      "metadata": {
        "id": "phQ6oAOqLAVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "\n",
        "def crtclass(mdTp,num_classes=24):\n",
        "  if mdTp == 'Res18':\n",
        "    modelF = models.resnet18()\n",
        "    modelF.fc = nn.Linear(512, num_classes)\n",
        "  elif mdTp == 'Res34':\n",
        "    modelF = models.resnet34()\n",
        "    modelF.fc = nn.Linear(512, num_classes)\n",
        "  elif mdTp == 'Res50':\n",
        "    modelF = models.resnet50()\n",
        "    modelF.fc = nn.Linear(2048, num_classes)\n",
        "  elif mdTp == 'Alex':\n",
        "    modelF = models.alexnet()\n",
        "    modelF.classifier[6] = nn.Linear(4096,num_classes)\n",
        "  elif mdTp == 'VGG':\n",
        "    modelF = models.vgg16()\n",
        "    modelF.classifier[6] = nn.Linear(4096,num_classes)\n",
        "  elif mdTp == 'Squeez':\n",
        "    modelF = models.squeezenet1_0()\n",
        "    modelF.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "  elif mdTp == 'Modile_v3':\n",
        "    modelF = models.mobilenet_v3_small()\n",
        "    modelF.classifier[0] = nn.Linear(in_features=576, out_features=4096, bias=True)\n",
        "    modelF.classifier[3] = nn.Linear(4096,num_classes,bias=True)\n",
        "  else:\n",
        "    assert False, 'no model'\n",
        "  return modelF\n",
        "\n",
        "def train_step(model,idata,ilabels,criterion,optimizer,DEVICE):\n",
        "    model.zero_grad()\n",
        "    idata = idata.type(torch.FloatTensor).to(DEVICE)\n",
        "    ilabels = ilabels.type(torch.FloatTensor).to(DEVICE)\n",
        "    train_output = model(idata)\n",
        "    #optimizer.zero_grad()\n",
        "    loss = criterion(train_output, ilabels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def test_acc(test_loader,model,DEVICE):\n",
        "    def sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    model.eval()\n",
        "    acc_array=[]\n",
        "    labels_output = []\n",
        "    labels_true = []\n",
        "    with torch.no_grad():\n",
        "      for i_test_data, i_test_labels in test_loader:\n",
        "            i_test_data = i_test_data.type(torch.FloatTensor).to(DEVICE)\n",
        "            i_test_labels = i_test_labels.detach().numpy()\n",
        "            test_output = model(i_test_data)\n",
        "            test_output = test_output.cpu().detach().numpy()\n",
        "\n",
        "            labels_true+=list(i_test_labels)\n",
        "            labels_output+=list(test_output)\n",
        "\n",
        "            test_label = np.argmax(test_output,axis=1)\n",
        "            true_label = np.argmax(i_test_labels,axis=1)\n",
        "            acc_array.extend((test_label==true_label))\n",
        "    labels_true = np.array(labels_true)\n",
        "    labels_output = sigmoid(np.array(labels_output))\n",
        "    try:\n",
        "      score = label_ranking_average_precision_score(labels_true, labels_output)\n",
        "    except:\n",
        "      score = 0.0\n",
        "    return (np.sum(acc_array)/len(acc_array)),score\n",
        "\n",
        "class simpleDataset(Dataset):\n",
        "    def __init__(self, numpy_dataset,numpy_labels,transform=None, target_transform=None):\n",
        "        self.dataset = numpy_dataset\n",
        "        self.labels = numpy_labels\n",
        "        self.length = len(numpy_labels)\n",
        "        self.target_transform = target_transform\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform != None:\n",
        "          data = self.transform(self.dataset[idx])\n",
        "        else:\n",
        "          data = self.dataset[idx]\n",
        "        if self.target_transform != None:\n",
        "          target = self.target_transform(self.labels[idx])\n",
        "        else:\n",
        "          target = self.labels[idx]\n",
        "        return data, target"
      ],
      "metadata": {
        "id": "A868LNVpK_gZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xzGE8FLnLMuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Обучение"
      ],
      "metadata": {
        "id": "B5omQ-xyLQW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = 0.1\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "LR = 1e-1\n",
        "LR_red = 1e-2\n",
        "\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCH_NUM = 30\n",
        "\n",
        "update_best_models_Flag = True\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOFmoYC_LSia",
        "outputId": "c162e676-8fa8-4127-8f79-c376bfbaa392"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка данных"
      ],
      "metadata": {
        "id": "8rpUir0sLbuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DIR = '/content/drive/MyDrive/mldm21_project/'\n",
        "PATH_TO_SAVE = '/content/drive/MyDrive/mldm21_project/'\n",
        "\n",
        "data_tr = np.load(PATH_TO_SAVE+'pure_train.npz')\n",
        "data_ts = np.load(PATH_TO_SAVE+'pure_test.npz')\n",
        "data_mts = np.load(PATH_TO_SAVE+'MAJOR_test.npz')\n",
        "\n",
        "train_data = data_tr['data']\n",
        "train_labels = data_tr['labels']\n",
        "test_data = data_ts['data']\n",
        "test_labels = data_ts['labels']\n",
        "majot_test_data = data_mts['data']"
      ],
      "metadata": {
        "id": "cn6MFlxnLbDu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = simpleDataset(train_data,train_labels)\n",
        "test_dataset = simpleDataset(test_data,test_labels)"
      ],
      "metadata": {
        "id": "n5j6R31LS6uY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "EjZ_ciXOVAH5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание сеток"
      ],
      "metadata": {
        "id": "asUseKHjUPFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model_18 = crtclass('Res18').to(DEVICE)\n",
        "best_model_18 = crtclass('Res18')\n",
        "criterion_18 = nn.BCEWithLogitsLoss()\n",
        "optimizer_18 = optim.SGD(model_18.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "model_50 = crtclass('Res50').to(DEVICE)\n",
        "best_model_50 = crtclass('Res50')\n",
        "criterion_50 = nn.BCEWithLogitsLoss()\n",
        "optimizer_50 = optim.SGD(model_50.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "\n",
        "\n",
        "models_arr = [model_18,model_50]\n",
        "\n",
        "criterion_arr = [criterion_18,criterion_50]\n",
        "optimizer_arr = [optimizer_18,optimizer_50]\n",
        "\n",
        "num_models = len(models_arr)"
      ],
      "metadata": {
        "id": "m3eFWlJ6UOgL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models_arr = [best_model_18,best_model_50]\n",
        "ModelNames = ['Res18','Res50']"
      ],
      "metadata": {
        "id": "nCQaxYCoUalw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy \n",
        "from IPython import display\n",
        "\n",
        "\n",
        "loss_arr = []\n",
        "test_acc_array = [ [] for i in range(num_models)]\n",
        "test_score_array = [ [] for i in range(num_models)]\n",
        "max_acc = [ 0 for i in range(num_models)]\n",
        "\n",
        "for iepoch in tqdm(range(EPOCH_NUM)):\n",
        "  inter_loss_arr = []\n",
        "  for imodel in models_arr:\n",
        "    imodel.train()\n",
        "  for idata, ilabels in train_loader:\n",
        "    cur_loss_arr = [ [] for i in range(num_models)]\n",
        "    intI=0\n",
        "    for imodel,icriterion ,ioptimizer in zip(models_arr,criterion_arr,optimizer_arr):\n",
        "      cur_loss = train_step(imodel,idata,ilabels,icriterion,ioptimizer,DEVICE)\n",
        "      cur_loss_arr[intI] = cur_loss\n",
        "      intI+=1\n",
        "    inter_loss_arr.append(cur_loss_arr)\n",
        "  loss_arr.append(np.mean(inter_loss_arr,axis=0))\n",
        "  cur_test_acc = []\n",
        "  intI=0\n",
        "\n",
        "  for imodel,ibest_model in zip(models_arr,best_models_arr):\n",
        "    acc_cur,score_cur = test_acc(test_loader,imodel,DEVICE)\n",
        "    test_acc_array[intI].append(acc_cur)\n",
        "    test_score_array[intI].append(score_cur)\n",
        "    cur_test_acc.append(acc_cur)\n",
        "    if (acc_cur > max_acc[intI]) and (update_best_models_Flag):\n",
        "      max_acc[intI]=acc_cur\n",
        "      sd = imodel.state_dict()\n",
        "      ibest_model.load_state_dict(sd)\n",
        "    intI+=1\n",
        "\n",
        "  strOUT_1 = 'eph.: {:} '.format(iepoch)\n",
        "  strOUT_2 = 'loss:' + str(loss_arr[-1])\n",
        "  strOUT_3 = 'eph.acc. : '+ str(cur_test_acc)\n",
        "  strOUT_4 =  'best acc. : '+ str(max_acc)\n",
        "  print(strOUT_1)\n",
        "  print(strOUT_2)\n",
        "  print(strOUT_3)\n",
        "  print(strOUT_4)\n",
        "    \n",
        "\n",
        "\n",
        "  # plot figures in epoch\n",
        "  loss_arr_np = np.array(loss_arr)\n",
        "  test_acc_array_np = np.array(test_acc_array)\n",
        "  test_score_array_np = np.array(test_score_array)\n",
        "\n",
        "  plt.figure(figsize=(18,4))\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.suptitle(strOUT_1+'   '+strOUT_2)\n",
        "  plt.title('loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('loss')\n",
        "  for idx,inamne in enumerate(ModelNames):\n",
        "    plt.plot(loss_arr_np[:,idx],label=inamne)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.title('acc. top-1')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('test acc.')\n",
        "  for idx,inamne in enumerate(ModelNames):\n",
        "    plt.plot(test_acc_array_np[idx],label=inamne)\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1,3,3)\n",
        "  plt.title('score LRAP')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('test score')\n",
        "  for idx,inamne in enumerate(ModelNames):\n",
        "    plt.plot(test_score_array_np[idx],label=inamne)\n",
        "  plt.legend()\n",
        "\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(plt.gcf())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "EZf91XVTUgPY",
        "outputId": "6b676c8b-6969-45bc-aeb0-4e77cb236b4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/30 [01:26<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-be0bf14fa24d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mintI\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0micriterion\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mioptimizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0milabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0micriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mioptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mcur_loss_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintI\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mintI\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-62d568b7a1f6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, idata, ilabels, criterion, optimizer, DEVICE)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tGh0qiKdVBu8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}